{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Adversarial AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    DEVICE = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (DEVICE, ))\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def show(filename):\n",
    "    from IPython import display\n",
    "    try:\n",
    "        display.display(\n",
    "            display.Image(filename=filename)\n",
    "        )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def one_hot(y, n_classes=10):\n",
    "    y_ = np.zeros(shape=(y.shape[0], n_classes), dtype='float32')\n",
    "    y_[np.arange(y.shape[0]), y] = 1\n",
    "    \n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "ds_train = MNIST(\"../../data/\", train=True, download=True)\n",
    "ds_test = MNIST(\"../../data/\", train=False, download=True)\n",
    "\n",
    "data_train = \\\n",
    "    ds_train.data.reshape(-1, 1, 28, 28).detach().numpy().astype(np.float32) / 255\n",
    "\n",
    "labels_train = ds_train.targets.detach().numpy()\n",
    "\n",
    "### to make everything fast we transfer the entire dataset into GPU\n",
    "X_train = torch.tensor(data_train, dtype=torch.float32, device=DEVICE)\n",
    "y_train = torch.tensor(labels_train, dtype=torch.long, device=DEVICE)\n",
    "y_one_hot_train = torch.tensor(one_hot(labels_train), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "X_avg = torch.mean(X_train, dim=0)\n",
    "MSE_baseline = torch.mean((X_train - X_avg[None, :, :, :]) ** 2)\n",
    "\n",
    "data_test = \\\n",
    "    ds_test.data.reshape(-1, 1, 28, 28).detach().numpy().astype(np.float32) / 255\n",
    "\n",
    "labels_test = ds_test.targets.detach().numpy()\n",
    "\n",
    "X_test = torch.tensor(data_test, dtype=torch.float32, device=DEVICE)\n",
    "y_test = torch.tensor(labels_test, dtype=torch.long, device=DEVICE)\n",
    "y_one_hot_test = torch.tensor(one_hot(labels_test), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "dataset_test = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_test, y_test),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(\n",
    "    np.concatenate(\n",
    "        np.concatenate(data_train[:200].reshape(20, 10, 28, 28), axis=2),\n",
    "        axis=0\n",
    "    ),\n",
    "    cmap=plt.cm.Greys\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class View(torch.nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Fully convolutional network\n",
    "### aka encoder\n",
    "### E: (None, 1, 28, 28), (None, additional_z_size) -> (None, code_size)\n",
    "class Inference(torch.nn.Module):\n",
    "    def __init__(self, n, code_size, xi_size=None):\n",
    "        super(Inference, self).__init__()\n",
    "        \n",
    "        self.image_embedding = [\n",
    "            ### 26 x 26\n",
    "            torch.nn.Conv2d(1, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n",
    "            ### 24 x 24\n",
    "            torch.nn.Conv2d(2 * n, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n",
    "            ### 12 x 12, conv pooling\n",
    "            torch.nn.Conv2d(2 * n, 2 * n, kernel_size=2, stride=2), torch.nn.LeakyReLU(),\n",
    "\n",
    "            ### 10 x 10\n",
    "            torch.nn.Conv2d(2 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n",
    "            ### 8 x 8\n",
    "            torch.nn.Conv2d(3 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n",
    "            ### 4 x 4, conv pooling\n",
    "            torch.nn.Conv2d(3 * n, 3 * n, kernel_size=2, stride=2), torch.nn.LeakyReLU(),\n",
    "            \n",
    "\n",
    "            ### 2 x 2\n",
    "            torch.nn.Conv2d(3 * n, 4 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n",
    "            ### 1 x 1\n",
    "            torch.nn.Conv2d(4 * n, code_size, kernel_size=2, stride=1),\n",
    "\n",
    "            torch.nn.Flatten()\n",
    "        ]\n",
    "        \n",
    "        for i, f in enumerate(self.image_embedding):\n",
    "            self.add_module('img_embedding%d' % (i, ), f)\n",
    "        \n",
    "        xi_size = 0 if xi_size is None else xi_size\n",
    "        \n",
    "        self.combined = [\n",
    "            torch.nn.Linear(code_size + xi_size, 2 * code_size), torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(2 * code_size, code_size)\n",
    "        ]\n",
    "        \n",
    "        for i, f in enumerate(self.combined):\n",
    "            self.add_module('combined%d' % (i, ), f)\n",
    "\n",
    "    def forward(self, x, z=None):\n",
    "        for f in self.image_embedding:\n",
    "            x = f(x)\n",
    "        \n",
    "        if z is not None:\n",
    "            x = torch.cat([x, z], dim=1)\n",
    "        \n",
    "        for f in self.combined:\n",
    "            x = f(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### aka decoder\n",
    "### G: (None, code_size) -> (None, 1, 28, 28)\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, n, code_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.modules = [\n",
    "            torch.nn.Linear(code_size, 4 * n),\n",
    "            View(-1, 4 * n, 1, 1),\n",
    "            \n",
    "            ### 2 x 2\n",
    "            torch.nn.ConvTranspose2d(4 * n, 4 * n, kernel_size=2, stride=1),\n",
    "            ### 4 x 4\n",
    "            torch.nn.ConvTranspose2d(4 * n, 3 * n, kernel_size=3, stride=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            ### 8 x 8\n",
    "            torch.nn.ConvTranspose2d(3 * n, 3 * n, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            ### 10 x 10\n",
    "            torch.nn.ConvTranspose2d(3 * n, 3 * n, kernel_size=3, stride=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            ### 12 x 12\n",
    "            torch.nn.ConvTranspose2d(3 * n, 2 * n, kernel_size=3, stride=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            ### 24 x 24\n",
    "            torch.nn.ConvTranspose2d(2 * n, 2 * n, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            ### 26 x 26\n",
    "            torch.nn.ConvTranspose2d(2 * n, 2 * n, kernel_size=3, stride=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            ### 28 x 28\n",
    "            torch.nn.ConvTranspose2d(2 * n, 1, kernel_size=3, stride=1),\n",
    "        ]\n",
    "        \n",
    "        for i, f in enumerate(self.modules):\n",
    "            self.add_module('f%d' % (i, ), f)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = z\n",
    "\n",
    "        for f in self.modules:\n",
    "            x = f(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### aka critic\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, n, input_size, n_outputs=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.fs = [\n",
    "            torch.nn.Linear(input_size, 2 * n),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            \n",
    "            torch.nn.Linear(2 * n, n),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        ]\n",
    "        \n",
    "        if n_outputs is None:\n",
    "            self.fs.append(torch.nn.Linear(n, 1))\n",
    "        else:\n",
    "            self.fs.append(torch.nn.Linear(n, n_outputs))\n",
    "                \n",
    "        \n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        for i, f in enumerate(self.fs):\n",
    "            self.add_module('f%d' % (i, ), f)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for f in self.fs:\n",
    "            x = f(x)\n",
    "        \n",
    "        if self.n_outputs is None:\n",
    "            return x.view(-1)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def iterate(f, n_epoches, n_steps, callback=None):\n",
    "    losses = np.zeros((n_epoches, n_batches), dtype=np.float32)\n",
    "\n",
    "    primary_pbar = tqdm(total=n_epoches, leave=False)\n",
    "    secondary_pbar = tqdm(total=n_steps, leave=False)\n",
    "\n",
    "    for i in range(n_epoches):\n",
    "        secondary_pbar.reset()\n",
    "\n",
    "        for j in range(n_steps):\n",
    "            losses[i, j] = f()\n",
    "\n",
    "            secondary_pbar.update()\n",
    "\n",
    "        primary_pbar.update()\n",
    "        if callback is not None:\n",
    "            callback()\n",
    "\n",
    "    secondary_pbar.close()\n",
    "    primary_pbar.close()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "### discriminator is a light-weight network, thus,\n",
    "### can easily handle large batches\n",
    "batch_size_discr_real = 32\n",
    "batch_size_discr_prior = 128\n",
    "\n",
    "n_epoches = 4\n",
    "\n",
    "n_batches = len(data_train) // batch_size\n",
    "\n",
    "n = 16\n",
    "code_size = 16\n",
    "xi_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def logit_binary_crossentropy(predictions_positive, predictions_negative):\n",
    "    \"\"\"\n",
    "    Accepts logits (output of a network before sigmoid or softmax) and returns cross-entropy loss.\n",
    "    - predictions_positive - predictions on real samples (y = 1);\n",
    "    - predictions_negative - predictions on generated samples (y = 0);\n",
    "    \"\"\"\n",
    "\n",
    "    ### -log sigmoid(p) = log( 1 + exp(-p) ) = softplus(-p)\n",
    "    return torch.mean(\n",
    "        torch.nn.functional.softplus(-predictions_positive)\n",
    "    ) + torch.mean(\n",
    "        torch.nn.functional.softplus(predictions_negative)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1\n",
    "\n",
    "- implement training procedure for the discriminator;\n",
    "- implement training procedure for the autoencoder.\n",
    "\n",
    "![AAE](../../img/AAE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e978b1ade30137b9f65e8c6cf3295c83",
     "grade": false,
     "grade_id": "AAE-discriminator",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def get_step_discriminator(inference, discriminator, opt_discriminator):\n",
    "    def step():\n",
    "        with torch.no_grad():\n",
    "            indx = torch.randint(low=0, high=X_train.shape[0], size=(batch_size_discr_real, ), device=DEVICE)\n",
    "            X_real = X_train[indx]\n",
    "            \n",
    "            ### xi makes inference stochastic\n",
    "            xi = torch.randn(X_real.shape[0], xi_size, device=DEVICE)\n",
    "            Z_inferred = inference(X_real, xi)\n",
    "            \n",
    "            Z_prior = torch.randn(batch_size_discr_prior, code_size, device=DEVICE)\n",
    "        \n",
    "        opt_discriminator.zero_grad()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        loss.backward()\n",
    "        opt_discriminator.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47b4b0f397f4ed7d52e7206936aee9b4",
     "grade": false,
     "grade_id": "AAE-AE",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "def get_step_AE(generator, inference, opt_AE, discriminator, alpha=1e-1):\n",
    "    def step():\n",
    "        with torch.no_grad():\n",
    "            indx = torch.randint(low=0, high=X_train.shape[0], size=(batch_size, ), device=DEVICE)\n",
    "            X_real = X_train[indx]\n",
    "            xi = torch.randn(X_real.shape[0], xi_size, device=DEVICE)\n",
    "        \n",
    "        opt_AE.zero_grad()\n",
    "        \n",
    "        ### loss now consists of two term - mse and penalty\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_AE.step()\n",
    "\n",
    "        return mse.item()\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_step_AAE(step_discriminator, step_AE, discriminator_steps = 4):\n",
    "    def step():\n",
    "        for _ in range(discriminator_steps):\n",
    "            step_discriminator()\n",
    "\n",
    "        return step_AE()\n",
    "    \n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Building Generator and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "generator = Generator(n, code_size).to(DEVICE)\n",
    "inference = Inference(n, code_size, xi_size).to(DEVICE)\n",
    "discriminator = Discriminator(n, code_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### checks if shapes are correct\n",
    "X_real = X_train[:10]\n",
    "xi = torch.randn(10, xi_size, device=DEVICE)\n",
    "\n",
    "Z_inferred = inference(X_real, xi)\n",
    "\n",
    "print('Z inferred shape', Z_inferred.shape)\n",
    "\n",
    "X_generated = generator(Z_inferred)\n",
    "\n",
    "print('X generated shape:', X_generated.shape)\n",
    "\n",
    "p_neg = discriminator(Z_inferred)\n",
    "\n",
    "print('discriminator shape:', p_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def inspect():\n",
    "    m = 20\n",
    "    with torch.no_grad():\n",
    "        Z = torch.randn(m, code_size, device=DEVICE) \n",
    "        X_gen = generator(Z)\n",
    "        \n",
    "        xi = torch.randn(m, xi_size, device=DEVICE)\n",
    "        X_rec = generator(inference(X_gen, xi))\n",
    "        \n",
    "        X_original = X_train[:m]\n",
    "        xi = torch.randn(m, xi_size, device=DEVICE) \n",
    "        X_rec_original = generator(inference(X_original, xi))\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(m * 2, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(\n",
    "        np.concatenate(\n",
    "            np.concatenate([\n",
    "                    X_gen.cpu().numpy().reshape(m, 28, 28),\n",
    "                    X_rec.cpu().numpy().reshape(m, 28, 28),\n",
    "                    X_original.cpu().numpy().reshape(m, 28, 28),\n",
    "                    X_rec_original.cpu().numpy().reshape(m, 28, 28),\n",
    "                ],\n",
    "                axis=1\n",
    "            ),\n",
    "            axis=1\n",
    "        ),\n",
    "        vmin=0, vmax=1,\n",
    "        cmap=plt.cm.Greys\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### note that we use separate optimizers for pretraining as\n",
    "### optimization tasks are different.\n",
    "opt_AE = torch.optim.Adam(\n",
    "    lr=2e-4,\n",
    "    params=list(inference.parameters()) + list(generator.parameters()),\n",
    ")\n",
    "\n",
    "opt_discr = torch.optim.Adam(\n",
    "    lr=2e-3, weight_decay=1e-3,\n",
    "    params=discriminator.parameters(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "step_discr = get_step_discriminator(inference, discriminator, opt_discr)\n",
    "\n",
    "### pretraining discriminator\n",
    "_ = iterate(step_discr, 8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "step_AE = get_step_AE(\n",
    "    generator, inference,\n",
    "    opt_AE=opt_AE,\n",
    "    discriminator=discriminator,\n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "step_adv = get_step_AAE(\n",
    "    step_discriminator=step_discr,\n",
    "    step_AE=step_AE,\n",
    "    discriminator_steps=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "losses_AAE = iterate(\n",
    "    step_adv, n_epoches=4, n_steps=n_batches,\n",
    "    callback=inspect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(np.mean(losses_AAE, axis=1))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "AAE-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
   ],
   "source": [
    "codes = list()\n",
    "errors = list()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in dataset_test:\n",
    "        xi = torch.randn(X_batch.shape[0], xi_size, device=DEVICE)\n",
    "        z = inference(X_batch, xi)\n",
    "        codes.append(z.cpu().numpy())\n",
    "        X_rec = generator(z)\n",
    "        errors.append(\n",
    "            torch.mean(\n",
    "                ((X_rec - X_batch) ** 2).view(X_batch.shape[0], -1) / MSE_baseline,\n",
    "                dim=1\n",
    "            ).cpu().numpy()\n",
    "        )\n",
    "\n",
    "codes = np.concatenate(codes, axis=0)\n",
    "errors = np.concatenate(errors, axis=0)\n",
    "\n",
    "show('../../img/AAE-1.png')\n",
    "\n",
    "if np.mean(errors) > 0.5:\n",
    "    raise ValueError('Reconstruction error is too high [%.2lf]!' % (np.mean(errors), ))\n",
    "else:\n",
    "    show('../../img/AAE-2.png')\n",
    "\n",
    "if np.any(np.abs(np.mean(codes, axis=0)) > 2.5e-1):\n",
    "    raise ValueError('Latent variables are biased!\\nmean = %s' % (np.mean(codes, axis=1)))\n",
    "elif np.any(np.std(codes, axis=0) > 1.5) or np.any(np.std(codes, axis=0) < 0.5):\n",
    "    raise ValueError('The variance of latent variables is too high!\\nstd = %s' % (np.std(codes, axis=1)))\n",
    "else:\n",
    "    show('../../img/AAE-3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.scatter(np.arange(code_size), np.mean(codes, axis=0))\n",
    "plt.errorbar(np.arange(code_size), np.mean(codes, axis=0), np.std(codes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "_ = plt.hist(\n",
    "    [codes[:, i] for i in range(code_size)],\n",
    "    bins=50,\n",
    "    histtype='step'\n",
    ")\n",
    "plt.title('Distribution of latent variables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}