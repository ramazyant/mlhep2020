{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bayesian Optimization: scalable edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    DEVICE = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (DEVICE, ))\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot_gp(gp, X=None, y=None, objective=None):\n",
    "    with torch.no_grad():\n",
    "        xs_ = np.linspace(0, 1, 100)\n",
    "        mean, std =  gp(\n",
    "            torch.tensor(xs_.reshape(-1, 1), device=DEVICE, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    mean_ = mean.cpu().numpy()\n",
    "    std_ = std.cpu().numpy()\n",
    "    \n",
    "    if X is not None and y is not None:\n",
    "        plt.scatter(X, y, color=plt.cm.tab10(0), s=100)\n",
    "    \n",
    "    if objective is not None:\n",
    "        plt.plot(xs_, objective(xs_), '--', color='black')\n",
    "    \n",
    "    plt.plot(xs_, mean_, lw=2)\n",
    "    plt.fill_between(\n",
    "        xs_, mean_ - std_, mean_ + std_,\n",
    "        alpha=0.2,\n",
    "        color=plt.cm.tab10(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A very special case of GP --- linear non-kernel version of GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class LinGP(torch.nn.Module):\n",
    "  def __init__(self, sigma_w=1, sigma_b=1, sigma_n=0.5):\n",
    "    \"\"\"\n",
    "    Linear Gaussian Process:\n",
    "        y ~ N(W X + b, (sigma_n) ** 2)\n",
    "        W_i ~ N(0, (sigma_w) ** 2)\n",
    "        b ~ N(0, (sigma_b) ** 2)\n",
    "\n",
    "    X, y = observed values, X - tensor of size (N, d), y - tensor of size (N, ).\n",
    "    sigma_w - variance of the prior over W.\n",
    "    sigma_b - variance of the prior over b.\n",
    "    sigma_n - variance of the noise.\n",
    "\n",
    "\n",
    "    Similar to GP with sklearn.gaussian_process.kernels.DotProduct with:\n",
    "        sigma_b0 = sigma_b;\n",
    "        sigma_0_bounds=(sigma_b, sigma_b),\n",
    "        alpha (GP parameter) = sigma_n ** 2\n",
    "\n",
    "    DotProduct kernel assumes sigma_w = 1.\n",
    "    \"\"\"\n",
    "    super(LinGP, self).__init__()\n",
    "\n",
    "    self.sigma_w = sigma_w\n",
    "    self.sigma_b = sigma_b\n",
    "    self.sigma_n = sigma_n\n",
    "\n",
    "    self.M = torch.zeros(0, 0, dtype=torch.float32)\n",
    "    self.A_inv_sigma = torch.zeros(0, 0, dtype=torch.float32)\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    X = torch.cat([\n",
    "      torch.ones(\n",
    "        X.shape[0], 1,\n",
    "        device=X.device, dtype=X.dtype,\n",
    "        requires_grad=X.requires_grad\n",
    "      ) * self.sigma_b / self.sigma_w,\n",
    "      X\n",
    "    ], dim=1)\n",
    "\n",
    "    cov_w = torch.eye(\n",
    "      n=X.shape[1],\n",
    "      device=X.device, dtype=X.dtype,\n",
    "      requires_grad=False\n",
    "    ) * ((self.sigma_n / self.sigma_w) ** 2)\n",
    "\n",
    "    A = torch.matmul(X.t(), X) + cov_w\n",
    "\n",
    "    A_inv = torch.inverse(A)\n",
    "\n",
    "    self.M = torch.matmul(\n",
    "      y,\n",
    "      torch.matmul(X, A_inv)\n",
    "    )\n",
    "\n",
    "    self.A_inv_sigma = A_inv * (self.sigma_n ** 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.cat([\n",
    "      torch.ones(\n",
    "        x.shape[0], 1,\n",
    "        device=x.device, dtype=x.dtype,\n",
    "        requires_grad=x.requires_grad\n",
    "      ) * self.sigma_b / self.sigma_w,\n",
    "      x\n",
    "    ], dim=1)\n",
    "\n",
    "    mean = torch.sum(x * self.M[None, :], dim=(1,))\n",
    "    sigma_sqr = torch.sum(\n",
    "      torch.matmul(x, self.A_inv_sigma) * x,\n",
    "      dim=(1,)\n",
    "    )\n",
    "\n",
    "    return mean, torch.sqrt(sigma_sqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def simple_objective(x):\n",
    "    return np.exp(-2 * x + 1) + np.exp(x) - 2.5\n",
    "\n",
    "X = np.array([0.1, 0.4, 0.9])\n",
    "y = simple_objective(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "N_BASIS = 7\n",
    "def basis_expansion(x):\n",
    "    return np.stack([\n",
    "        x ** i\n",
    "        for i in range(N_BASIS)\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lin_gp = LinGP(sigma_n=1e-1, sigma_b=1).to(DEVICE)\n",
    "lin_gp.fit(\n",
    "    torch.tensor(basis_expansion(X), dtype=torch.float32, device=DEVICE),\n",
    "    torch.tensor(y.reshape(-1), dtype=torch.float32, device=DEVICE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def predict_lin_gp(X):\n",
    "    with torch.no_grad():\n",
    "        X = torch.cat([\n",
    "            X ** i\n",
    "            for i in range(N_BASIS)\n",
    "        ], dim=1)\n",
    "        return lin_gp(X)\n",
    "    \n",
    "plot_gp(predict_lin_gp, X, y, objective=simple_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's compare it to pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pyro\n",
    "pyro.set_rng_seed(111222333)\n",
    "\n",
    "from pyro.contrib.gp.models import GPRegression\n",
    "from pyro.contrib.gp.kernels import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_large = torch.randn(1024, 32, dtype=torch.float32, device=DEVICE)\n",
    "y_large = torch.randn(1024, dtype=torch.float32, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "\n",
    "lin_gp.fit(X_large, y_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "\n",
    "pyro.clear_param_store()\n",
    "gp = GPRegression(\n",
    "    X_large, y_large,\n",
    "\n",
    "    noise=torch.tensor(1e-2),\n",
    "    kernel=Linear(\n",
    "        input_dim=1,\n",
    "        variance=torch.tensor(1, dtype=torch.float32, device=DEVICE),\n",
    "    ),\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = gp(X_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Network with multiple head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class NeuralGP(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n, lin_gp):\n",
    "        super(NeuralGP, self).__init__()\n",
    "\n",
    "        self._basis = [\n",
    "            torch.nn.Linear(n_inputs, 4 * n), torch.nn.ELU(),\n",
    "            torch.nn.Linear(4 * n, 2 * n), torch.nn.ELU(),\n",
    "            torch.nn.Linear(2 * n, n), torch.nn.ELU(),\n",
    "        ]\n",
    "        \n",
    "        for i, f in enumerate(self._basis):\n",
    "            self.add_module('f%d' % (i, ), f)\n",
    "        \n",
    "        self.lin_reg = torch.nn.Linear(n, 1)\n",
    "        self.lin_gp = lin_gp\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.opt = torch.optim.Adam(\n",
    "            lr=2e-3, weight_decay=1e-3,\n",
    "            params=self.parameters()\n",
    "        )\n",
    "    \n",
    "    def basis(self, x, dropout=False):\n",
    "        for f in self._basis:\n",
    "            x = f(x)\n",
    "            \n",
    "            if dropout:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, dropout=False):\n",
    "        basis = self.basis(x, dropout=dropout)\n",
    "        return self.lin_reg(basis)\n",
    "    \n",
    "    def fit(self, X, y, n_iters=1024, progress_bar=lambda x: x):\n",
    "        ### training regression NN\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=DEVICE)\n",
    "        \n",
    "        losses = np.zeros(n_iters)\n",
    "    \n",
    "        for i in progress_bar(range(n_iters)):\n",
    "            with torch.no_grad():\n",
    "                indx = torch.randint(low=0, high=X.shape[0], size=(32, ), device=DEVICE)\n",
    "                X_batch = X[indx]\n",
    "                y_batch = y[indx]\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "            predictions = self.lin_reg(\n",
    "                self.basis(X_batch, dropout=True)\n",
    "            ).view(-1)\n",
    "\n",
    "            loss = torch.mean((predictions - y_batch) ** 2)\n",
    "\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "\n",
    "            losses[i] = loss.item()\n",
    "        \n",
    "        ### fitting linGP\n",
    "        with torch.no_grad():\n",
    "            basis = self.basis(X)\n",
    "            self.lin_gp.fit(basis, y)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def get_predictions(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "            return self.lin_reg(self.basis(X)).view(-1).cpu().numpy()\n",
    "    \n",
    "    def get_basis(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "            return self.basis(X).cpu().numpy()\n",
    "    \n",
    "    def gp(self, X):\n",
    "        basis = self.basis(X)\n",
    "        return self.lin_gp(basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simple objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lin_gp = LinGP(sigma_n=1e-1, sigma_b=1).to(DEVICE)\n",
    "\n",
    "nngp = NeuralGP(n_inputs=1, n=8, lin_gp=lin_gp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.array([0.1, 0.4, 0.9])\n",
    "y = simple_objective(X)\n",
    "\n",
    "nngp.fit(X.reshape(-1, 1), y, n_iters=1024, progress_bar=tqdm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot_gp(nngp.gp, X, y, objective=simple_objective)\n",
    "\n",
    "xs = np.linspace(0, 1, num=100)\n",
    "p = nngp.get_predictions(xs.reshape(-1, 1))\n",
    "plt.plot(xs, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-modal objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "m = 5\n",
    "w = np.random.uniform(-1, 1, size=m)\n",
    "periods = np.arange(1, m + 1) * np.pi\n",
    "\n",
    "def objective_many_minima_much_challenge(x):\n",
    "    return np.sum(\n",
    "        w[:, None] * np.cos(x[None, :] * periods[:, None]),\n",
    "        axis=0\n",
    "    ) + x ** 2\n",
    "\n",
    "xs = np.linspace(0, 1, num=100)\n",
    "plt.plot(xs, objective_many_minima_much_challenge(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.concatenate([\n",
    "    np.random.uniform(0, 0.45, size=10),\n",
    "    np.random.uniform(0.55, 1, size=10),\n",
    "])\n",
    "y = objective_many_minima_much_challenge(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lin_gp = LinGP(sigma_n=1, sigma_b=1).to(DEVICE)\n",
    "nngp = NeuralGP(n_inputs=1, n=8, lin_gp=lin_gp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "nngp.fit(X.reshape(-1, 1), y, n_iters=8 * 1024, progress_bar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot_gp(nngp.gp, X, y, objective=objective_many_minima_much_challenge)\n",
    "\n",
    "xs = np.linspace(0, 1, num=100)\n",
    "basis = nngp.get_basis(xs.reshape(-1, 1))\n",
    "\n",
    "for i in range(basis.shape[1]):\n",
    "    plt.plot(xs, basis[:, i], alpha=0.1, color=plt.cm.tab10(0))\n",
    "\n",
    "plt.plot(xs, nngp.get_predictions(xs.reshape(-1, 1)), lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils\n",
    "\n",
    "*stolen from the previous seminar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def minimize_acq(x0, gp, acq, lr=1e-1, n_iters=128, progress_bar=lambda x: x):\n",
    "    x = torch.tensor(x0, dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
    "    \n",
    "    ### not stochastic in this case\n",
    "    opt = torch.optim.SGD(lr=lr, params=[x])\n",
    "    \n",
    "    for _ in progress_bar(range(n_iters)):\n",
    "        opt.zero_grad()        \n",
    "        torch.sum(acq(gp, x)).backward()\n",
    "        opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        values = acq(gp, x)\n",
    "\n",
    "    return x.detach().cpu().numpy(), values.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Endurance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def objective(x):\n",
    "    return np.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def lcb(gp, x, alpha=1.0):\n",
    "    mean, std = gp(x)\n",
    "    \n",
    "    return mean - alpha * std\n",
    "\n",
    "### Purely explorative acquisition function.\n",
    "def exlorative(gp, x, alpha=1.0):\n",
    "    mean, std = gp(x)\n",
    "    return - std\n",
    "\n",
    "### Purely exploitative acquisition function.\n",
    "def exploitative(gp, x, alpha=1.0):\n",
    "    mean, std = gp(x)\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def cummin(fs):\n",
    "    result = np.zeros_like(fs)\n",
    "    \n",
    "    result[0] = fs[0]\n",
    "    \n",
    "    for i in range(1, fs.shape[0]):\n",
    "        if result[i - 1] < fs[i]:\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = fs[i]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot_convergence(y, color=plt.cm.tab10(0), label=None):\n",
    "    y = np.array(y)\n",
    "    iters = np.arange(len(y))\n",
    "    \n",
    "    plt.plot(iters, cummin(y), color=color, label=label)\n",
    "    plt.scatter(iters, y, alpha=0.5, color=color, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot(f, history=None, trajectory=False):\n",
    "    xs = np.linspace(0, 1, num=250)\n",
    "    X, Y = np.meshgrid(xs, xs)\n",
    "\n",
    "    Z = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    F = f(Z[:, 0], Z[:, 1]).reshape(xs.shape[0], xs.shape[0])\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "\n",
    "    plt.contour(\n",
    "        xs, xs, F,\n",
    "        levels=np.linspace(0, 4, num=20),\n",
    "        colors=[plt.cm.tab10(0)]\n",
    "    )\n",
    "    plt.scatter([1], [1], marker='*', s=500, color=plt.cm.tab10(3), zorder=5)\n",
    "    \n",
    "    if history is not None:\n",
    "        plt.scatter(history[:, 0], history[:, 1], color=plt.cm.tab10(1), s=100)\n",
    "        if trajectory:\n",
    "            plt.plot(history[:, 0], history[:, 1], color=plt.cm.tab10(1), lw=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def log_resenbrock(x1, x2):\n",
    "    x1 = x1 * 3 - 0.5\n",
    "    x2 = x2 * 3 - 0.5\n",
    "    \n",
    "    return np.log1p(\n",
    "        (1 - x1) ** 2 + 1 * (x2 - x1 ** 2) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Random Search\n",
    "\n",
    "*(as a baseline)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_rs = np.random.uniform(size=(32, 2))\n",
    "y_rs = log_resenbrock(X_rs[:, 0], X_rs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NN-BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def nnbo(objective, nngp, n_iters=32):\n",
    "    X = list()\n",
    "    y = list()\n",
    "\n",
    "    for _ in tqdm(range(n_iters)):\n",
    "        if len(X) < 5:\n",
    "            suggestion = np.random.uniform(size=2)\n",
    "        else:\n",
    "            suggestions, values = minimize_acq(\n",
    "                np.random.uniform(size=(32, 2)),\n",
    "                nngp.gp,\n",
    "                lcb\n",
    "            )\n",
    "            best = np.argmin(values)\n",
    "            suggestion = suggestions[best]\n",
    "        \n",
    "        value = objective(*suggestion)\n",
    "\n",
    "        X.append(suggestion)\n",
    "        y.append(value)\n",
    "\n",
    "        nngp.fit(np.array(X), np.array(y), n_iters=1024)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Rosenbrock test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lin_gp = LinGP(sigma_n=1, sigma_b=1).to(DEVICE)\n",
    "nngp = NeuralGP(n_inputs=2, n=16, lin_gp=lin_gp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X, y = nnbo(log_resenbrock, nngp, n_iters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot(log_resenbrock, history=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot_convergence(y, label='NN-BO', color=plt.cm.tab10(0))\n",
    "plot_convergence(y_rs, label='RS', color=plt.cm.tab10(1))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trial by ~~fire~~ HIGGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data = np.load('../../../share/HIGGS-small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all, y_all = data[:, 1:], data[:, 0]\n",
    "\n",
    "### casting labels to int\n",
    "y_all = np.where(y_all > 0.5, np.int(1), np.int(0))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.25)\n",
    "\n",
    "print('Train     :', X_train.shape, y_train.shape)\n",
    "print('Validation:', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_gb(n_trees, log_learning_rate):\n",
    "    n_trees = int(100 * n_trees + 1)\n",
    "    log_learning_rate = log_learning_rate * 6 - 6\n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "    return GradientBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_trees, max_depth=3,\n",
    "        subsample=0.1, random_state=123\n",
    "    )\n",
    "\n",
    "def gb_quality(n_trees, log_learning_rate):\n",
    "    clf = get_gb(n_trees, log_learning_rate)\n",
    "    \n",
    "    predictions = clf.fit(X_train, y_train).predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    error = 1 - roc_auc_score(y_val, predictions)\n",
    "    computational_penalty = 1e-1 * n_trees\n",
    "    \n",
    "    return error + computational_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lin_gp = LinGP(sigma_n=1e-1, sigma_b=1).to(DEVICE)\n",
    "nngp = NeuralGP(n_inputs=2, n=16, lin_gp=lin_gp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X, y = nnbo(gb_quality, nngp, n_iters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### looking for the best guess with an exploitative acqusition function\n",
    "suggestions, values = minimize_acq(\n",
    "    x0=np.random.uniform(0, 1, size=(128, 2)),\n",
    "    gp=nngp.gp,\n",
    "    n_iters=1024,\n",
    "    progress_bar=tqdm,\n",
    "    acq=exploitative\n",
    ")\n",
    "best = np.argmin(values)\n",
    "best_guess = suggestions[best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "clf = get_gb(*best_guess)\n",
    "    \n",
    "predictions = clf.fit(X_train, y_train).predict_proba(X_val)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_val, predictions)\n",
    "computational_penalty = 1e-1 * x[0]\n",
    "\n",
    "print('ROC AUC: %.3lf' % (auc, ))\n",
    "print('complexity: %.3lf' % (computational_penalty, ))\n",
    "print('objecive: %.3lf' % (1 - auc + computational_penalty))\n",
    "\n",
    "assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plot_convergence(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}